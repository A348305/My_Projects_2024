{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPrTULt/PDY1a9+0k3kMyuS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/A348305/My_Projects_2024/blob/main/My_mini_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8zA1D8jYhNY"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import davies_bouldin_score, silhouette_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import silhouette_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Data Loading and Preprocessing\n",
        "# Load your datasets\n",
        "df1 = pd.read_csv(\"https://raw.githubusercontent.com/A348305/mydatasets/master/ConfLongDemo_JSI.csv\")\n",
        "#print(df1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = pd.read_csv(\"https://raw.githubusercontent.com/A348305/mydatasets/master/kinetic%20data.csv\")\n",
        "#print(df2)"
      ],
      "metadata": {
        "id": "Ptc2L4PGZGK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3 = pd.read_csv(\"https://raw.githubusercontent.com/A348305/mydatasets/master/test.xlsb.csv\")\n",
        "#print(df3)"
      ],
      "metadata": {
        "id": "UVseFWCP_x0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_dataset = pd.concat([df1, df2, df3], axis=1)\n",
        "# Handling missing values\n",
        "combined_dataset.fillna(0, inplace=True)\n",
        "combined_dataset = combined_dataset[(combined_dataset != 0).all(axis=1)]\n",
        "print(combined_dataset)"
      ],
      "metadata": {
        "id": "gCFqlzVV5X1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZdyjKgMwQggx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing_values =combined_dataset.isna().sum()\n",
        "missing_percent = (combined_dataset.isna().sum() / len(combined_dataset)) * 100\n",
        "print(missing_values)\n",
        "print(missing_percent)"
      ],
      "metadata": {
        "id": "hX1cN1nHF9zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "znIi-fpSQcFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_cols = combined_dataset.select_dtypes(include=['float64', 'int64']).columns\n",
        "non_numeric_cols = combined_dataset.select_dtypes(exclude=['float64', 'int64']).columns\n",
        "\n",
        "# Label encoding for non-numeric data\n",
        "label_encoders = {}\n",
        "for col in non_numeric_cols:\n",
        "    # Convert all values in the column to string type\n",
        "    combined_dataset[col] = combined_dataset[col].astype(str)\n",
        "    le = LabelEncoder()\n",
        "    combined_dataset[col] = le.fit_transform(combined_dataset[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        " #Standardize numeric data\n",
        "scaler = StandardScaler()\n",
        "combined_dataset[numeric_cols] = scaler.fit_transform(combined_dataset[numeric_cols])\n",
        "\n",
        "\n",
        "# Elbow Method to find the optimal number of clusters\n",
        "inertia_values = []\n",
        "possible_k_values = range(1, 15)  # Check for up to 14 clusters\n",
        "\n",
        "for k in possible_k_values:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(combined_dataset[numeric_cols])  # Use only numeric columns for clustering\n",
        "    inertia_values.append(kmeans.inertia_)\n",
        "\n",
        "# Plot the Elbow Method graph\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(possible_k_values, inertia_values, marker='o', linestyle='--')\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('Elbow Method to Determine Optimal Cluster Number')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Compute the Correlation Matrix\n",
        "correlation_matrix = combined_dataset[numeric_cols].corr()\n",
        "\n",
        "# Visualize the Correlation Matrix using a Heatmap\n",
        "plt.figure(figsize=(15, 15))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, square=True, linewidths=0.5)\n",
        "plt.title('Feature Correlation Heatmap')\n",
        "plt.show()\n",
        "\n",
        "# Step 3: Apply a machine learning algorithm\n",
        "# Here, we'll use K-Means clustering as an example\n",
        "num_clusters = 4  # You can adjust the number of clusters as needed\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
        "combined_dataset['cluster'] = kmeans.fit_predict(combined_dataset[numeric_cols])  # Use only numeric columns for clustering\n",
        "\n",
        "# Step 4: Dimensionality reduction (optional)\n",
        "# You can use PCA or other dimensionality reduction techniques if needed\n",
        "pca = PCA(n_components=2)\n",
        "combined_dataset['pca_1'], combined_dataset['pca_2'] = pca.fit_transform(combined_dataset[numeric_cols]).T\n",
        "\n",
        "# Step 5: Save or analyze the results\n",
        "# The 'cluster' column now contains cluster labels for each data point\n",
        "# You can further analyze the clusters or use them for downstream tasks\n",
        "\n",
        "# Optional: Inverse transform for non-numeric data (if needed)\n",
        "for col, le in label_encoders.items():\n",
        "    combined_dataset[col] = le.inverse_transform(combined_dataset[col])\n",
        "\n",
        "# Optional: Inverse transform for numeric data (if needed)\n",
        "combined_dataset[numeric_cols] = scaler.inverse_transform(combined_dataset[numeric_cols])\n",
        "\n",
        "# Save the results to a new CSV file\n",
        "combined_dataset.to_csv('clustered_combined_dataset.csv', index=False)"
      ],
      "metadata": {
        "id": "lolvD3YnABiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the clusters after PCA\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.scatterplot(x='pca_1', y='pca_2', data=combined_dataset, hue='cluster', palette='viridis', s=60, edgecolor=None, alpha=0.7)\n",
        "plt.title('K-Means Clustering with PCA Visualization')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RIiBwuzaAO83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "davies_bouldin = davies_bouldin_score(combined_dataset[numeric_cols], combined_dataset['cluster'])\n",
        "print(f\"Davies-Bouldin Score: {davies_bouldin:.3f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "kwHsSGIPePrd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}